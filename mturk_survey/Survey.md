# Human Evaluation of Explanations
To evaluate the quality of explanations generated by OFA-X, we conducted a human evaluation on 
Amazon Mechanical Turk. We use the same setup as in the [e-ViL paper](https://openaccess.thecvf.com/content/ICCV2021/html/Kayser_E-ViL_A_Dataset_and_Benchmark_for_Natural_Language_Explanations_in_ICCV_2021_paper.html).

We evaluate a total of 5 different models. For each model, we randomly sample 300 examples from their respective test set
where the model predicted the task answer correctly.

## Evaluations
### General structure
Each assignment consists of 5 questions. Each question consists of a dataset specific task and the corresponding image.
The workers must select the correct answer from the given options. After selecting an answer, the worker must also rate,
for both ground truth and predicted explanation, whether the explanation justifies the answer.
The workers can choose between "Yes", "Weak yes", "No", "Weak no". These
scores are later mapped to an explanation score where yes = 1, weak yes = 2/3, weak no = 1/3, and no = 0.
They can, for each explanation, also choose any shortcoming they see in the explanation. These shortcomings are
"Incorrect description of the image", "Insufficient justification" and "Confusing sentence".
Finally, they are asked which explanation they prefer and can choose between "Explanation 1", "Explanation 2", or "No preference".

### Large Model VQA-X
We evaluate the large model (470M parameters) that was first pretrained as in the [OFA paper](http://arxiv.org/abs/2202.03052),
fine-tuned on Image Captioning, and then fine-tuned on VQA-X. We sample 300 examples from the test set of VQA-X where the
task answer was correct.
See `mturk_survey/data/vqa.csv` for the sampled examples and `mturk_survey/templates/vqax.html` for the html template.

The three answer candidates are the "correct" answer, and two other randomly sampled answers from the dataset.

### Large Model e-SNLI-VE
We evaluate the large model (470M parameters) that was first pretrained as in the [OFA paper](http://arxiv.org/abs/2202.03052)
and then fine-tuned on e-SNLI-VE. We sample 300 examples from the test set of e-SNLI-VE where the task answer was correct.
See `mturk_survey/data/esnlive.csv` for the sampled examples and `mturk_survey/templates/esnlive.html` for the html template.

### Large Model VCR
We evaluate the large model (470M parameters) that was first pretrained as in the [OFA paper](http://arxiv.org/abs/2202.03052)
and then fine-tuned on VCR. We sample 300 examples from the test set of VCR where the task answer was correct.
See `mturk_survey/data/vcr.csv` for the sampled examples and `mturk_survey/templates/vcr.html` for the html template.

### Large Unified Model (OFA-X<sub>MT</sub>)
We evaluate the large model (470M parameters) that was first pretrained as in the [OFA paper](http://arxiv.org/abs/2202.03052)
and then fine-tuned on all three datasets (VQA-X, e-SNLI-VE, VCR). We sample a total of 300 examples from the test sets of
VQA-X, e-SNLI-VE, and VCR where the task answer was correct. We sample 100 examples from each dataset. We use the maximum
overlap between the single dataset samples to ensure that the evaluations are as comparable as possible. For each dataset,
we use the same template as for the single dataset evaluations.
See `mturk_survey/data/unified_vqax.csv`, `mturk_survey/data/unified_esnlive.csv`, and `mturk_survey/data/unified_vcr.csv`
for the sampled examples.

### Huge Model VCR
To investigate the effect of model size, we also evaluate the huge model (970M parameters) that was 
first pretrained as in the [OFA paper](http://arxiv.org/abs/2202.03052)
and then fine-tuned on VCR. We sample 300 examples from the test set of VCR where the task answer was correct and again
use the maximum overlap between this model's samples and the samples of the large model.
See `mturk_survey/data/huge_vcr.csv` for the sampled examples and `mturk_survey/templates/vcr.html` for the html template.


## Results
The results for each evaluation are stored in `mturk_survey/results/`. The "raw" results from MTurk are stored as
`task_results_raw.csv`. The results are then processed using `evaluate_survey_results.py` to compute the final scores.
The resulting results both before and after filtering out examples where the worker selected the wrong answer are stored
as `task_results_processed.csv` and `task_results_processed_filtered.csv` respectively.

## Rejecting assignments
For the "Large Unified Model (OFA-X<sub>MT</sub>)" evaluation and the "Huge Model VCR" evaluation, we
rejected assignments where the worker got less than 3 of the 5 answers correct. This was necessary to ensure that the
workers understood the assignment. Moreover, this was necessary because the amount of these assignments was too high to
just filter out. Each rejected assignment was replaced by a new assignment. In the "raw" results, the rejected assignments
are marked with the "rejected" status. In the processed results, the rejected assignments are not included.